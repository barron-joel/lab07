---
title: "Lab 07 - Modelling course evaluations"
author: "Joel Barron"
date: "`r Sys.Date()`"
output: html_document
---

### Packages and Data

```{r load-packages, message=FALSE, echo=TRUE}
library(tidyverse)
library(tidymodels)

```


```{r read-data}
evals<-read.csv("data/evals.csv", row.names=1)
```


# Exercise 1: Exploratory Data Analysis

1.  Visualize the distribution of `score` in the dataframe `evals`.

```{r viz-score}
evals %>%
  ggplot(mapping = aes(x=score)) + 
  geom_histogram()

evals %>%
  summarise(
    avg_score = mean(score),
    avg_med_score = median(score),
    sd_score = sd(score)
  )
```

There is a skewed distribution towards the upper range of scores - the mean and median score are both around 4.3, and the standard deviation is reasonably low around it. This tells you students generally rate the lecturers quite highly. I would expect this, as University of Texas is a very respected university, and lecturers are likely to be very good. Scores are normally shifted up as well when rating, as few people perceive 2.5 as the true average, with an "average" rating being closer to 3.5.

2.  Visualize and describe the relationship between `score` and `bty_avg` using `geom_point()` to represent the data. 

```{r scatterplot}
evals %>%
  ggplot(mapping = aes(
    x = bty_avg,
    y = score
  )) +
  geom_point()

evals %>%
  ggplot(mapping = aes(
    x = bty_avg,
    y = score
  )) +
  geom_jitter()

```

Jitter "shakes" points around slightly so you can view the difference when there is multiple points. The jitter allows you to see that there is a higher density of points at the higher scores, which was not apparent on the simple point plot.

# Exercise 2: Simple Linear regression with a numerical predictor

1. Fit a linear model called `score_bty_fit` to predict average professor evaluation `score` from average beauty rating (`bty_avg`). Print the regression output using `tidy()`.

```{r fit-score_bty_fit}
score_bty_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ bty_avg, data = evals)
```

```{r tidy-score_bty_fit}
tidy(score_bty_fit)
```

score-hat = 3.88 + 0.066 x bty_avg

2. Plot the data again using `geom_jitter()`, and add the regression line.

```{r viz-score_bty_fit,eval=FALSE}
evals %>%
  ggplot(mapping = aes(
    x = bty_avg,
    y = score
  )) +
  geom_jitter() +
  geom_smooth(method = "lm", se=FALSE)
```

3. Interpret the slope of the linear model in context of the data.

When the bty_avg increases by 1, the score given to the professor will increase by 0.066.

4. Interpret the intercept of the linear model in context of the data. Comment on whether or not the intercept makes sense in this context.

If the bty_avg equals zero for a professor, you would expect a score of 3.88. The intercept does make sense in this context.

5. Determine the $R^2$ of the model and interpret it in the context of the data.

```{r R2}
glance(score_bty_fit)$adj.r.squared
```

As the value of R2 is close to zero, the predicted values from the model are quite far from the actual values observed in the dataset.

6. Make a plot of residuals vs. predicted values for the model above.

```{r viz-score_bty_fit-diagnostic}
score_bty_aug <- augment(score_bty_fit$fit)

ggplot(score_bty_aug, mapping = aes(
  x = .fitted,
  y = .resid
)) + 
  geom_jitter() +
  geom_hline(yintercept = 0, linetype = "dashed")
```
This linear model is appropriate as the residual plot is randomly distributed about 0.

# Exercise 3: Simple Linear regression with a categorical predictor

0. Look at the variable rank, and determine the frequency of each category level.

```{r}
evals %>%
  group_by(rank) %>%
  count()
```

1. Fit a new linear model called `score_rank_fit` to predict average professor evaluation `score` based on `rank` of the professor.

```{r fit-score_rank_fit}
score_rank_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ rank, data = evals)

tidy(score_rank_fit)
```

score-hat = 4.28 - 0.130 x rank_tenure_track - 0.145 x rank_tenured

The intercept is the predicted score if your rank is teaching. The slopes show that this decreases by 0.130 if you are rank tenrue track, and decreases by 0.145 if your rank is tenured.

2. Fit a new linear model called `score_gender_fit` to predict average professor evaluation `score` based on `gender` of the professor. 

```{r fit-score_gender_fit}
score_gender_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ gender, data = evals)

tidy(score_gender_fit)
```

```{r score_gender_intercept, echo=FALSE}
score_gender_intercept <- tidy(score_gender_fit) %>% 
  filter(term == "(Intercept)") %>%
  select(estimate) %>%
  pull()
```

```{r score_gender_slope, echo=FALSE}
score_gender_slope <- tidy(score_gender_fit) %>% 
  filter(term == "gendermale") %>%
  select(estimate) %>%
  pull()
```

The intercept of the model is `r round(score_gender_intercept, 3)`
The slope of this model is `r round(score_gender_slope, 3)`


# Exercise 4: Multiple linear regression

1. Fit a multiple linear regression model, predicting average professor evaluation `score` based on average beauty rating (`bty_avg`) and `gender.`

```{r fit-score_bty_gender_fit}
score_bty_gender_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(score ~ bty_avg + gender, data = evals)

tidy(score_bty_gender_fit)
```

The intercept is the score given to a female when her beauty score is zero. The slope for bty_avg is the increase in score for a increase of 1 in beauty rating. The slope for gender male is the increase in score if the professor is male.

```{r eval = FALSE}
ggplot(evals, mapping = aes(
  y = score,
  x = bty_avg,
  colour = gender
)) + 
  geom_jitter()
```

2. What percent of the variability in `score` is explained by the model `score_bty_gender_fit`. 

```{r}
# ...
```


3. What is the equation of the line corresponding to just male professors?

score-hat = 3.92 + 0.074 x bty_avg

4. For two professors who received the same beauty rating, which gender tends to have the higher course evaluation score?

Male

5. How does the relationship between beauty and evaluation score vary between male and female professors?

*Add your narrative here.*

6. How do the adjusted $R^2$ values of `score_bty_fit` and `score_bty_gender_fit` compare? 

```{r eval=FALSE}
# remove eval = FALSE from the code chunk options after filling in the blanks
glance(___)$adj.r.squared
glance(___)$adj.r.squared
```

*Add your narrative here.*

7. Compare the slopes of `bty_avg` under the two models (`score_bty_fit` and `score_bty_gender_fit`).

*Add your narrative here.*

# Exercise 5: Interpretation of log-transformed response variables

If you do not know how to use LaTeX, do this exercise with pen and paper.
